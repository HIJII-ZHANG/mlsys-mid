"""
ä»»åŠ¡4ï¼šç”¨Profileræ‰¾ç“¶é¢ˆ
ä½¿ç”¨PyTorch Profileråˆ†æè®­ç»ƒè¿‡ç¨‹ï¼Œæ‰¾åˆ°æœ€è€—æ—¶çš„æ“ä½œ
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.profiler as profiler
from mobilenet_v2 import get_mobilenet_v2


class RandomDataset(Dataset):
    """ç”Ÿæˆéšæœºæ•°æ®çš„æ•°æ®é›†"""
    def __init__(self, size=1000):
        self.size = size

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        # ç”Ÿæˆéšæœºå›¾åƒå’Œæ ‡ç­¾
        image = torch.randn(3, 32, 32)
        label = torch.randint(0, 10, (1,)).item()
        return image, label


def train_with_profiler(model, device, batch_size=64, num_workers=4, profile_steps=20):
    """ä½¿ç”¨Profilerè¿›è¡Œè®­ç»ƒåˆ†æ"""
    dataset = RandomDataset(size=1000)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=True
    )

    model.train()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    print(f"å¼€å§‹ä½¿ç”¨Profileråˆ†æè®­ç»ƒè¿‡ç¨‹...")
    print(f"é…ç½®: batch_size={batch_size}, num_workers={num_workers}")
    print(f"å°†åˆ†æå‰ {profile_steps} ä¸ªè®­ç»ƒæ­¥éª¤\n")

    # é…ç½®Profiler
    with profiler.profile(
        activities=[
            profiler.ProfilerActivity.CPU,
            profiler.ProfilerActivity.CUDA,
        ],
        schedule=profiler.schedule(
            wait=1,      # é¢„çƒ­1æ­¥
            warmup=2,    # çƒ­èº«2æ­¥
            active=3,    # æ´»è·ƒè®°å½•3æ­¥
            repeat=2     # é‡å¤2æ¬¡
        ),
        on_trace_ready=profiler.tensorboard_trace_handler('./profiler_logs'),
        record_shapes=True,      # è®°å½•å¼ é‡å½¢çŠ¶
        profile_memory=True,     # è®°å½•å†…å­˜ä½¿ç”¨
        with_stack=True          # è®°å½•Pythonå †æ ˆ
    ) as prof:

        step = 0
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            prof.step()  # é€šçŸ¥profileræ­¥éª¤å®Œæˆ
            step += 1

            if step % 5 == 0:
                print(f"  æ­¥éª¤ {step}/{profile_steps} å®Œæˆ")

            if step >= profile_steps:
                break

    print(f"\nâœ… Profilingå®Œæˆï¼")
    return prof


def analyze_profiler_results(prof):
    """åˆ†æProfilerç»“æœ"""
    print("\n" + "="*80)
    print("Profileråˆ†æç»“æœ")
    print("="*80 + "\n")

    # 1. æŒ‰CPUæ—¶é—´æ’åºçš„å‰10ä¸ªæ“ä½œ
    print("ã€Top 10 æœ€è€—CPUæ—¶é—´çš„æ“ä½œã€‘")
    print("-"*80)
    print(prof.key_averages().table(
        sort_by="cpu_time_total",
        row_limit=10,
        max_src_column_width=50
    ))

    # 2. æŒ‰CUDAæ—¶é—´æ’åºçš„å‰10ä¸ªæ“ä½œ
    print("\nã€Top 10 æœ€è€—GPUæ—¶é—´çš„æ“ä½œã€‘")
    print("-"*80)
    print(prof.key_averages().table(
        sort_by="cuda_time_total",
        row_limit=10,
        max_src_column_width=50
    ))

    # 3. æŒ‰å†…å­˜ä½¿ç”¨æ’åº
    print("\nã€Top 10 æœ€è€—æ˜¾å­˜çš„æ“ä½œã€‘")
    print("-"*80)
    print(prof.key_averages().table(
        sort_by="self_cuda_memory_usage",
        row_limit=10,
        max_src_column_width=50
    ))

    # 4. å¯¼å‡ºChrome traceæ–‡ä»¶
    trace_file = "profiler_trace.json"
    prof.export_chrome_trace(trace_file)
    print(f"\nğŸ“Š Chrome traceæ–‡ä»¶å·²å¯¼å‡º: {trace_file}")
    print(f"   æŸ¥çœ‹æ–¹æ³•ï¼š")
    print(f"   1. æ‰“å¼€Chromeæµè§ˆå™¨")
    print(f"   2. è®¿é—® chrome://tracing")
    print(f"   3. ç‚¹å‡» 'Load' æŒ‰é’®åŠ è½½ {trace_file}")

    # 5. ç”Ÿæˆåˆ†æå»ºè®®
    print("\n" + "="*80)
    print("åˆ†æå»ºè®®")
    print("="*80)

    key_averages = prof.key_averages()

    # æ‰¾å‡ºæœ€è€—æ—¶çš„æ“ä½œ
    cpu_ops = sorted(key_averages, key=lambda x: x.cpu_time_total, reverse=True)[:3]
    cuda_ops = sorted(key_averages, key=lambda x: x.cuda_time_total, reverse=True)[:3]

    print("\nğŸ” æœ€è€—æ—¶çš„æ“ä½œï¼š")
    print(f"\nCPUç«¯ï¼š")
    for i, op in enumerate(cpu_ops, 1):
        print(f"  {i}. {op.key}: {op.cpu_time_total/1000:.2f}ms")

    print(f"\nGPUç«¯ï¼š")
    for i, op in enumerate(cuda_ops, 1):
        print(f"  {i}. {op.key}: {op.cuda_time_total/1000:.2f}ms")

    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼š")

    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®åŠ è½½ç“¶é¢ˆ
    data_ops = [op for op in key_averages if 'DataLoader' in op.key or 'data' in op.key.lower()]
    if data_ops and sum(op.cpu_time_total for op in data_ops) > 1000000:  # >1s
        print("  âš ï¸  æ•°æ®åŠ è½½å¯èƒ½æ˜¯ç“¶é¢ˆï¼Œå»ºè®®ï¼š")
        print("     - å¢åŠ  num_workers")
        print("     - ä½¿ç”¨ pin_memory=True")
        print("     - è€ƒè™‘æ•°æ®é¢„å¤„ç†ä¼˜åŒ–")

    # æ£€æŸ¥æ˜¯å¦æœ‰CPUåˆ°GPUæ•°æ®ä¼ è¾“ç“¶é¢ˆ
    copy_ops = [op for op in key_averages if 'copy' in op.key.lower() or 'to' in op.key.lower()]
    if copy_ops and sum(op.cuda_time_total for op in copy_ops) > 500000:  # >0.5s
        print("  âš ï¸  æ•°æ®ä¼ è¾“å¯èƒ½æ˜¯ç“¶é¢ˆï¼Œå»ºè®®ï¼š")
        print("     - ä½¿ç”¨ pin_memory=True")
        print("     - ä½¿ç”¨ non_blocking=True")
        print("     - å‡å°‘CPU-GPUæ•°æ®ä¼ è¾“é¢‘ç‡")

    # æ£€æŸ¥å·ç§¯æ“ä½œ
    conv_ops = [op for op in key_averages if 'conv' in op.key.lower()]
    if conv_ops:
        total_conv_time = sum(op.cuda_time_total for op in conv_ops)
        total_time = sum(op.cuda_time_total for op in key_averages)
        if total_time > 0:
            conv_ratio = total_conv_time / total_time * 100
            print(f"  â„¹ï¸  å·ç§¯æ“ä½œå GPUæ—¶é—´çš„ {conv_ratio:.1f}%")
            if conv_ratio > 60:
                print("     - è¿™æ˜¯æ­£å¸¸çš„ï¼Œå·ç§¯æ˜¯è®¡ç®—å¯†é›†å‹æ“ä½œ")
                print("     - å¯ä»¥è€ƒè™‘ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ(AMP)åŠ é€Ÿ")

    print("\nğŸ“ˆ åœ¨Chrome Tracingä¸­æŸ¥çœ‹ï¼š")
    print("  - è“è‰²æ¡ï¼šCPUæ“ä½œ")
    print("  - ç»¿è‰²æ¡ï¼šGPUæ“ä½œï¼ˆCUDA kernelsï¼‰")
    print("  - å¦‚æœCPUå’ŒGPUæ—¶é—´æ¡æ²¡æœ‰é‡å ï¼Œè¯´æ˜å­˜åœ¨äº’ç›¸ç­‰å¾…")
    print("  - å¯»æ‰¾æœ€é•¿çš„æ—¶é—´æ¡ï¼Œé‚£å°±æ˜¯ä¸»è¦ç“¶é¢ˆ")

    print("\n" + "="*80 + "\n")


def export_tensorboard_logs():
    """æç¤ºå¦‚ä½•æŸ¥çœ‹TensorBoardæ—¥å¿—"""
    print("\nğŸ“Š TensorBoardåˆ†æï¼š")
    print("="*80)
    print("é™¤äº†Chrome traceï¼Œè¿˜å¯ä»¥ä½¿ç”¨TensorBoardæŸ¥çœ‹æ›´è¯¦ç»†çš„åˆ†æï¼š")
    print("\nè¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨TensorBoardï¼š")
    print("  tensorboard --logdir=./profiler_logs")
    print("\nç„¶ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ï¼š")
    print("  http://localhost:6006")
    print("\nTensorBoardæä¾›ï¼š")
    print("  - æ“ä½œçº§åˆ«çš„æ€§èƒ½åˆ†æ")
    print("  - å†…å­˜ä½¿ç”¨æ—¶é—´çº¿")
    print("  - GPUåˆ©ç”¨ç‡ç»Ÿè®¡")
    print("  - ç®—å­çº§åˆ«çš„æ‰§è¡Œæ—¶é—´åˆ†å¸ƒ")
    print("="*80 + "\n")


def run_task4():
    """æ‰§è¡Œä»»åŠ¡4ï¼šProfileråˆ†æ"""
    print("\n" + "="*80)
    print("ä»»åŠ¡4ï¼šç”¨Profileræ‰¾ç“¶é¢ˆ")
    print("="*80 + "\n")

    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("è­¦å‘Š: CUDAä¸å¯ç”¨ï¼ŒProfileråˆ†æåœ¨CPUä¸Šè¿›è¡Œ")
        device = torch.device("cpu")
    else:
        device = torch.device("cuda:0")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"GPU: {torch.cuda.get_device_name(0)}")

    print()

    # åˆ›å»ºæ¨¡å‹
    model = get_mobilenet_v2(num_classes=10).to(device)
    params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹: MobileNetV2")
    print(f"å‚æ•°é‡: {params:,} ({params/1e6:.2f}M)\n")

    # è¿è¡ŒProfiler
    prof = train_with_profiler(
        model=model,
        device=device,
        batch_size=64,
        num_workers=4,
        profile_steps=20
    )

    # åˆ†æç»“æœ
    analyze_profiler_results(prof)

    # TensorBoardæç¤º
    export_tensorboard_logs()

    print("æç¤ºï¼š")
    print("  1. æŸ¥çœ‹ profiler_trace.json æ–‡ä»¶äº†è§£è¯¦ç»†çš„æ‰§è¡Œæ—¶åº")
    print("  2. åœ¨Chrome Tracingä¸­å¯ä»¥ç¼©æ”¾å’Œå¹³ç§»æŸ¥çœ‹ä¸åŒæ—¶é—´æ®µ")
    print("  3. ç‚¹å‡»å…·ä½“æ“ä½œå¯ä»¥çœ‹åˆ°å‚æ•°å’Œå †æ ˆä¿¡æ¯")
    print("  4. å¯¹æ¯”ä¸åŒé…ç½®ï¼ˆbatch_size, num_workersç­‰ï¼‰çš„traceæ–‡ä»¶\n")


if __name__ == "__main__":
    run_task4()
